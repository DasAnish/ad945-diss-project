{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_diss_anish.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d99fbecb4b64dc5b9c5d9494ca5757d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b5e3959c47134fab9a784acca8d83e82",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6070faad522144dda072d0341830913e",
              "IPY_MODEL_e6d46ac6f426440c8de9f78ea3699e6f"
            ]
          }
        },
        "b5e3959c47134fab9a784acca8d83e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6070faad522144dda072d0341830913e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e0583df15e8948e1bdda5703d32a30b1",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4cdc0b3718a4ed9bb3c7b1881bacd2c"
          }
        },
        "e6d46ac6f426440c8de9f78ea3699e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8de8639f38884c7eae7a5fac62d15361",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/10 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c14436d66d3f49df8e71a79c762d6018"
          }
        },
        "e0583df15e8948e1bdda5703d32a30b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4cdc0b3718a4ed9bb3c7b1881bacd2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8de8639f38884c7eae7a5fac62d15361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c14436d66d3f49df8e71a79c762d6018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "113f0d6c130246368672054e82fd4bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_38d48f3db137434095f051c534cb65b7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3e3c4cee871c41b5984b34c6881bfc91",
              "IPY_MODEL_3a1238139a204c96accede8c6e505f83"
            ]
          }
        },
        "38d48f3db137434095f051c534cb65b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e3c4cee871c41b5984b34c6881bfc91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0e2768bea3f4b2a955cdd4647d23a7e",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 35390,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c450486bc52a4e3399d742219e50c326"
          }
        },
        "3a1238139a204c96accede8c6e505f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a869b0d37934bc38ba66a9838a660a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28/35390 [00:08&lt;1:44:08,  5.66it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a3d181d6ee740a1ad6248445cc97553"
          }
        },
        "d0e2768bea3f4b2a955cdd4647d23a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c450486bc52a4e3399d742219e50c326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a869b0d37934bc38ba66a9838a660a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a3d181d6ee740a1ad6248445cc97553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4b006oMPNuc",
        "outputId": "7bbd6e56-9152-4518-d39c-08eb624da7b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "print(os.getcwd())\n",
        "if os.getcwd() != '/content/drive/My Drive/Dissertation':\n",
        "    os.chdir('/content/drive/My Drive/Dissertation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNJVOFfNIunV",
        "outputId": "2b783b45-21e7-4013-a8a0-43eff1fde6bf"
      },
      "source": [
        "!pip install sacrebleu\r\n",
        "!pip install sentencepiece\r\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 10kB 24.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 20kB 16.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 30kB 13.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 40kB 12.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 6.0MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.2.1 sacrebleu-1.5.0\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 9.2MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y53o2dev0d2"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from math import sqrt\r\n",
        "Tensor = torch.Tensor\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "from typing import Optional\r\n",
        "import datetime\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "\r\n",
        "import sentencepiece as spm\r\n",
        "from tqdm import tqdm\r\n",
        "from tqdm.notebook import tnrange\r\n",
        "import sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSoN3YQZqU14"
      },
      "source": [
        "# % cd sentencepiece/\r\n",
        "# % cd build/\r\n",
        "# ! cmake ..\r\n",
        "# ! make -j $(nproc)\r\n",
        "# ! sudo make install\r\n",
        "# ! sudo ldconfig -v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lVNwQgIqghE"
      },
      "source": [
        "# % cd data/\r\n",
        "# % cd data/es"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkxLhpj7vGIs"
      },
      "source": [
        "# ! unzip en-es.txt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5MBRfa95Mi7"
      },
      "source": [
        "# spm.__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma6ZTbc-8D6E"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# import subprocess\r\n",
        "# # os.chdir('data')\r\n",
        "# subprocess.run(\r\n",
        "#             [\"unzip\", \"en-es.txt.zip\"],\r\n",
        "#             stdout=subprocess.PIPE,\r\n",
        "#             encoding=\"utf-8\"\r\n",
        "#         ).stdout.replace('â€˜', '\\'').replace('â€™', '\\'')\r\n",
        "# os.chdir('..')\r\n",
        "# taking first million\r\n",
        "\r\n",
        "# def move(l, num):\r\n",
        "#     inpfile = open(input_file+l, 'r', encoding=\"utf-8\")\r\n",
        "#     intfile = open(interim_file+l, 'w', encoding='utf-8')\r\n",
        "#     with mosestokenizer.MosesPuntuationNormalizer(l) as normalizer:\r\n",
        "#         for i in range(num):\r\n",
        "#             intfile.write(normalizer(inpfile.readline()).lower())\r\n",
        "#     inpfile.close()\r\n",
        "#     intfile.close()\r\n",
        "# move(\"fr\", 10**6)\r\n",
        "# move(\"en\", 10**6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fg8F_5534Et"
      },
      "source": [
        "# input_file = interim_file\r\n",
        "# vocab_size = 8000\r\n",
        "\r\n",
        "# for l in ['fr', 'en']:\r\n",
        "#     if not os.path.exists(f\"{model_file}{l}.model\"):\r\n",
        "#         spm.SentencePieceTrainer.Train(\r\n",
        "#             input=f\"{input_file}{l}\",\r\n",
        "#             model_prefix=f\"{model_file}{l}\",\r\n",
        "#             model_type='bpe',\r\n",
        "#             vocab_size=vocab_size,\r\n",
        "#             pad_id=3,\r\n",
        "#             pad_piece='<p>',\r\n",
        "#             bos_piece='<s>',\r\n",
        "#             eos_piece='</s>'\r\n",
        "#             )\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKK-OZfuv2vg"
      },
      "source": [
        "from math import sin, cos, sqrt\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, model_dim: int, heads: int, dropout: float = 0.1):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.model_dim: int = model_dim\r\n",
        "        self.heads: int = heads\r\n",
        "        self.d_k: int = model_dim // heads\r\n",
        "        self.dropout: nn.Dropout = nn.Dropout(dropout).to(device)\r\n",
        "        self.add_module('dropout', self.dropout)\r\n",
        "\r\n",
        "        self.w_key: nn.Linear = nn.Linear(model_dim, model_dim).to(device)\r\n",
        "        self.w_query: nn.Linear = nn.Linear(model_dim, model_dim).to(device)\r\n",
        "        self.w_value: nn.Linear = nn.Linear(model_dim, model_dim).to(device)\r\n",
        "        self.add_module('w_key', self.w_key)\r\n",
        "        self.add_module('w_query', self.w_query)\r\n",
        "        self.add_module('w_value', self.w_value)\r\n",
        "\r\n",
        "        self.final_layer: nn.Linear = nn.Linear(model_dim, model_dim).to(device)\r\n",
        "        self.add_module('final_layer', self.final_layer)\r\n",
        "\r\n",
        "    def forward(self,\r\n",
        "                q: Tensor,\r\n",
        "                k: Tensor,\r\n",
        "                v: Tensor,\r\n",
        "                mask: Optional[Tensor] = None):\r\n",
        "\r\n",
        "        # if encoder_output is not None: print(\"enc_output == True\")\r\n",
        "        # print(f'x.shape: {x.shape}')\r\n",
        "\r\n",
        "        batch_size = q.size(0)\r\n",
        "        queries = self.w_query(q).view(batch_size, -1, self.heads, self.d_k)\r\n",
        "        keys = self.w_key(k).view(batch_size, -1, self.heads, self.d_k)\r\n",
        "        values = self.w_value(v).view(batch_size, -1, self.heads, self.d_k)\r\n",
        "\r\n",
        "        # we have batch_size, seq_len, heads, d_k tensor\r\n",
        "\r\n",
        "        keys.transpose_(1, 2)\r\n",
        "        queries.transpose_(1, 2)\r\n",
        "        values.transpose_(1, 2)\r\n",
        "\r\n",
        "        interim_result = torch.matmul(queries, keys.transpose(-2, -1)) / sqrt(self.d_k)\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            mask = mask.unsqueeze(1)\r\n",
        "            interim_result = interim_result.masked_fill(mask == 0, -1e9)\r\n",
        "\r\n",
        "        interim_result = F.softmax(interim_result, dim=-1)\r\n",
        "\r\n",
        "        if self.dropout is not None:\r\n",
        "            interim_result = self.dropout(interim_result)\r\n",
        "\r\n",
        "        interim_result = torch.matmul(interim_result, values)\r\n",
        "\r\n",
        "        concatenated_result = interim_result.transpose(1, 2).contiguous().view(batch_size, -1, self.model_dim)\r\n",
        "\r\n",
        "        final_result = self.final_layer(concatenated_result)\r\n",
        "\r\n",
        "        return final_result\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class PositionalEncoding(nn.Module):\r\n",
        "\r\n",
        "    \"\"\"A parameter less module that concatenates a number of sine signals at the end of the embedded vectors.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, model_dim: int, max_length: int, dropout: float = 0.1):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.model_dim: int = model_dim\r\n",
        "        self.dropout: nn.Dropout = nn.Dropout(dropout)\r\n",
        "        self.add_module('dropout', self.dropout)\r\n",
        "\r\n",
        "        position_vector: Tensor = torch.zeros(max_length, model_dim, requires_grad=False).to(device)\r\n",
        "        # arange = torch.arange(max_length)\r\n",
        "\r\n",
        "        # note to self: this appear to work right now.\r\n",
        "        for pos in range(max_length):\r\n",
        "            for i in range(0, model_dim, 2):\r\n",
        "                # Follwing the formula provided in the paper.\r\n",
        "                position_vector[pos, i] = sin(pos / (10000 ** ((2 * i) / model_dim)))\r\n",
        "                position_vector[pos, i+1] = cos(pos / (10000 ** ((2 * (i + 1)) / model_dim)))\r\n",
        "\r\n",
        "        # position_vector: max_seq_len x model_dim\r\n",
        "        position_vector = position_vector.unsqueeze(0)\r\n",
        "\r\n",
        "        # position_vector: 1 x max_seq_len x model_dim\r\n",
        "        self.register_buffer('position_vector', position_vector)\r\n",
        "\r\n",
        "    def forward(self, x: Tensor) -> Tensor:\r\n",
        "        \"\"\"\r\n",
        "        The forward pass implementation of the Positional Embedding step.\r\n",
        "        :param x: the tensor containing Batch x Seq_len x model_dim embeddings.\r\n",
        "        :return: the embedded vector with some alterations.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        x = x * sqrt(self.model_dim)\r\n",
        "        sequence_length = x.size(1)\r\n",
        "        x = x + Variable(self.position_vector[:, :sequence_length], requires_grad=False)\r\n",
        "        x = self.dropout(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class FeedForward(nn.Module):\r\n",
        "\r\n",
        "    \"\"\"The utility module that is used to implement a feed-forward fully connected Neural Net for the\r\n",
        "    encoder and decoder layers.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, model_dim: int, d_ff: int = 2048, dropout: float = 0.1):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.fc1: nn.Linear = nn.Linear(model_dim, d_ff).to(device)\r\n",
        "        self.dropout: nn.Dropout = nn.Dropout(dropout).to(device)\r\n",
        "        self.fc2: nn.Linear = nn.Linear(d_ff, model_dim).to(device)\r\n",
        "\r\n",
        "        self.add_module('fc1', self.fc1)\r\n",
        "        self.add_module('dropout', self.dropout)\r\n",
        "        self.add_module('fc2', self.fc2)\r\n",
        "\r\n",
        "    def forward(self, x: Tensor) -> Tensor:\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        Implements Feed-Forward algorithm with dropout.\r\n",
        "        :param x: a tensor with last dim = model_dim\r\n",
        "        :return: output from the NN\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\r\n",
        "        x = self.fc2(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class TransformerEncoderLayer(nn.Module):\r\n",
        "    def __init__(self, model_dim: int, heads: int, d_ff: int,\r\n",
        "                 dropout: Optional[float] = 0.1, norm_before: bool = False):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.model_dim: int = model_dim\r\n",
        "        self.heads: int = heads\r\n",
        "        self.dropout: float = dropout\r\n",
        "        self.norm_before: bool = norm_before\r\n",
        "\r\n",
        "        self.self_attn: MultiHeadAttention = MultiHeadAttention(model_dim,\r\n",
        "                                                                heads,\r\n",
        "                                                                dropout).to(device)\r\n",
        "        self.add_module('self_attn', self.self_attn)\r\n",
        "\r\n",
        "        self.attn_dropout: nn.Dropout = nn.Dropout(dropout).to(device)\r\n",
        "        self.ffn_dropout: nn.Dropout = nn.Dropout(dropout).to(device)\r\n",
        "        self.add_module('attn_dropout', self.attn_dropout)\r\n",
        "        self.add_module('ffn_dropout', self.ffn_dropout)\r\n",
        "\r\n",
        "        self.ffn: FeedForward = FeedForward(model_dim, d_ff, dropout).to(device)\r\n",
        "        self.add_module('ffn', self.ffn)\r\n",
        "\r\n",
        "        self.attn_norm: nn.LayerNorm = nn.LayerNorm(model_dim).to(device)\r\n",
        "        self.ffn_norm: nn.LayerNorm = nn.LayerNorm(model_dim).to(device)\r\n",
        "        self.add_module('attn_norm', self.attn_norm)\r\n",
        "        self.add_module('ffn_norm', self.ffn_norm)\r\n",
        "\r\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\r\n",
        "        y = x\r\n",
        "        if self.norm_before:\r\n",
        "            y = self.attn_norm(x)\r\n",
        "        # print('encoder_attention')\r\n",
        "        y = self.self_attn(y, y, y, mask)\r\n",
        "        y = self.attn_dropout(y)\r\n",
        "        x = x + y\r\n",
        "        if not self.norm_before:\r\n",
        "            x = self.attn_norm(x)\r\n",
        "\r\n",
        "        y = x\r\n",
        "        if self.norm_before:\r\n",
        "            y = self.ffn_norm(x)\r\n",
        "        y = self.ffn(y)\r\n",
        "        y = self.ffn_dropout(y)\r\n",
        "        x = x + y\r\n",
        "        if not self.norm_before:\r\n",
        "            x = self.ffn_norm(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class TransformerEncoder(nn.Module):\r\n",
        "    def __init__(self, model_dim: int, d_ff: int, heads: int, num_blocks: int,\r\n",
        "                 vocab_size: int, max_seq_len: Optional[int] = 80,\r\n",
        "                 dropout: Optional[float] = 0.1, norm_before: bool = False):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.model_dim: int = model_dim\r\n",
        "        self.heads: int = heads\r\n",
        "        self.num_blocks: int = num_blocks\r\n",
        "        self.vocab_size: int = vocab_size\r\n",
        "        self.max_seq_len: int = max_seq_len\r\n",
        "\r\n",
        "        self.word_embeddings: nn.Embedding = nn.Embedding(vocab_size, model_dim).to(device)\r\n",
        "        self.positional_encoding: PositionalEncoding = PositionalEncoding(model_dim, max_seq_len).to(device)\r\n",
        "        self.add_module('word_embeddings', self.word_embeddings)\r\n",
        "\r\n",
        "        args = (model_dim, heads, d_ff, dropout, norm_before)\r\n",
        "        # encoding_layer: TransformerEncoderLayer = TransformerEncoderLayer(*args).to(device)\r\n",
        "        self.encoding_layers: List[TransformerEncoderLayer] = [TransformerEncoderLayer(*args).to(device)\r\n",
        "                                                               for _ in range(num_blocks)]\r\n",
        "        for i, layer in enumerate(self.encoding_layers):\r\n",
        "            self.add_module(f'layer_{i}', layer)\r\n",
        "\r\n",
        "    def forward(self, _input: Tensor, mask: Tensor):\r\n",
        "        x = self.word_embeddings(_input)\r\n",
        "        x = self.positional_encoding(x)\r\n",
        "        # print(f\"x.shape {x.shape}\")\r\n",
        "        for layer in self.encoding_layers:\r\n",
        "            x = layer(x, mask)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class TransformerDecoderLayer(nn.Module):\r\n",
        "    def __init__(self, model_dim: int, heads: int, d_ff: int,\r\n",
        "                 dropout: Optional[float] = 0.1, norm_before: bool = False):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.model_dim: int = model_dim\r\n",
        "        self.heads: int = heads\r\n",
        "        self.d_ff: int = d_ff\r\n",
        "        self.dropout: float = dropout\r\n",
        "        self.norm_before: bool = norm_before\r\n",
        "\r\n",
        "        self.self_attn_norm: nn.LayerNorm = nn.LayerNorm(model_dim).to(device)\r\n",
        "        self.enc_dec_norm: nn.LayerNorm = nn.LayerNorm(model_dim).to(device)\r\n",
        "        self.ffn_norm: nn.LayerNorm = nn.LayerNorm(model_dim).to(device)\r\n",
        "        self.add_module('self_attn_norm', self.self_attn_norm)\r\n",
        "        self.add_module('enc_dec_norm', self.enc_dec_norm)\r\n",
        "        self.add_module('ffn_norm', self.ffn_norm)\r\n",
        "\r\n",
        "        self.self_attn_dropout: nn.Dropout = nn.Dropout(dropout).to(device)\r\n",
        "        self.enc_dec_dropout: nn.Dropout = nn.Dropout(dropout).to(device)\r\n",
        "        self.ffn_dropout: nn.Dropout = nn.Dropout(dropout).to(device)\r\n",
        "        self.add_module('self_attn_dropout', self.self_attn_dropout)\r\n",
        "        self.add_module('enc_dec_dropout', self.self_attn_dropout)\r\n",
        "        self.add_module('ffn_dropout', self.ffn_dropout)\r\n",
        "\r\n",
        "        self.self_attn: MultiHeadAttention = MultiHeadAttention(model_dim, heads, dropout).to(device)\r\n",
        "        self.enc_dec_attn: MultiHeadAttention = MultiHeadAttention(model_dim, heads, dropout).to(device)\r\n",
        "        self.ffn: FeedForward = FeedForward(model_dim, d_ff).to(device)\r\n",
        "        self.add_module('self_attn', self.self_attn)\r\n",
        "        self.add_module('enc_dec_attn', self.enc_dec_attn)\r\n",
        "        self.add_module('ffn', self.ffn)\r\n",
        "\r\n",
        "    def forward(self, x: Tensor, encoder_output: Tensor, src_mask: Tensor, trg_mask: Tensor) -> Tensor:\r\n",
        "\r\n",
        "        y = x\r\n",
        "        if self.norm_before:\r\n",
        "            y = self.self_attn_norm(x)\r\n",
        "        # print('decoder_attention')\r\n",
        "        y = self.self_attn(y, y, y, trg_mask)\r\n",
        "        y = self.self_attn_dropout(y)\r\n",
        "        x = x + y\r\n",
        "        if not self.norm_before:\r\n",
        "            x = self.self_attn_norm(x)\r\n",
        "\r\n",
        "        y = x\r\n",
        "        if self.norm_before:\r\n",
        "            y = self.enc_dec_norm(x)\r\n",
        "        # print('enc_dec_attn')\r\n",
        "        y = self.enc_dec_attn(y, encoder_output, encoder_output,\r\n",
        "                              src_mask)\r\n",
        "        y = self.enc_dec_dropout(y)\r\n",
        "        x = x + y\r\n",
        "        if not self.norm_before:\r\n",
        "            x = self.enc_dec_norm(x)\r\n",
        "\r\n",
        "        y = x\r\n",
        "        if self.norm_before:\r\n",
        "            y = self.ffn_norm(x)\r\n",
        "        y = self.ffn(y)\r\n",
        "        y = self.ffn_dropout(y)\r\n",
        "        x = x + y\r\n",
        "        if not self.norm_before:\r\n",
        "            x = self.ffn_norm(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class TransformerDecoder(nn.Module):\r\n",
        "    def __init__(self, model_dim: int, d_ff: int, heads: int, num_blocks: int,\r\n",
        "                 vocab_size: int, max_seq_len: Optional[int] = 80,\r\n",
        "                 dropout: Optional[float] = 0.1, norm_before: bool = True):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.model_dim: int = model_dim\r\n",
        "        self.heads: int = heads\r\n",
        "        self.num_blocks: int = num_blocks\r\n",
        "        self.vocab_size: int = vocab_size\r\n",
        "        self.max_seq_len: int = max_seq_len\r\n",
        "        self.dropout: float = dropout\r\n",
        "        self.norm_before: bool = norm_before\r\n",
        "\r\n",
        "        self.word_embeddings: nn.Embedding = nn.Embedding(vocab_size, model_dim).to(device)\r\n",
        "        self.positional_embeddings: PositionalEncoding = PositionalEncoding(model_dim, max_seq_len).to(device)\r\n",
        "\r\n",
        "        args = (model_dim, heads, d_ff, dropout, norm_before)\r\n",
        "\r\n",
        "        # decoding_layer: TransformerDecoderLayer = TransformerDecoderLayer(*args).to(device)\r\n",
        "        self.decoding_layers: List[TransformerDecoderLayer] = [TransformerDecoderLayer(*args).to(device)\r\n",
        "                                                               for _ in range(num_blocks)]\r\n",
        "        for i, layer in enumerate(self.decoding_layers):\r\n",
        "            self.add_module(f'layer_{i}', layer)\r\n",
        "\r\n",
        "    def forward(self, target: Tensor, encoder_output: Tensor, src_mask: Tensor, trg_mask: Tensor) -> Tensor:\r\n",
        "        x: Tensor = self.word_embeddings(target)\r\n",
        "        x = self.positional_embeddings(x)\r\n",
        "\r\n",
        "        for layer in self.decoding_layers:\r\n",
        "            x = layer(x, encoder_output, src_mask, trg_mask)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, src_vocab_size: int, trg_vocab_size: int,\r\n",
        "                 model_dim: int, d_ff: int, heads: int, num_blocks: int,\r\n",
        "                 max_seq_len: Optional[int] = 80, dropout: Optional[float] = 0.1,\r\n",
        "                 norm_before: bool = False):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            dev = \"cuda:0\"\r\n",
        "        else:\r\n",
        "            dev = \"cpu\"\r\n",
        "\r\n",
        "        device = torch.device(dev)\r\n",
        "\r\n",
        "        self.encoder: TransformerEncoder = TransformerEncoder(model_dim, d_ff, heads, num_blocks,\r\n",
        "                                                              src_vocab_size, max_seq_len, dropout,\r\n",
        "                                                              norm_before).to(device)\r\n",
        "        self.decoder: TransformerDecoder = TransformerDecoder(model_dim, d_ff, heads, num_blocks,\r\n",
        "                                                              trg_vocab_size, max_seq_len, dropout,\r\n",
        "                                                              norm_before).to(device)\r\n",
        "\r\n",
        "        self.linear: nn.Linear = nn.Linear(model_dim, trg_vocab_size).to(device)\r\n",
        "\r\n",
        "        self.add_module('encoder', self.encoder)\r\n",
        "        self.add_module('decoder', self.decoder)\r\n",
        "        self.add_module('linear', self.linear)\r\n",
        "\r\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor, trg_mask: Tensor):\r\n",
        "        enc_output = self.encoder(src, src_mask)\r\n",
        "        dec_output = self.decoder(trg, enc_output, src_mask, trg_mask)\r\n",
        "        output = self.linear(dec_output)\r\n",
        "        del enc_output, dec_output\r\n",
        "        torch.cuda.empty_cache()\r\n",
        "        return output\r\n",
        "\r\n",
        "    def save_model(self, file_name):\r\n",
        "      torch.save(self.state_dict(), file_name)\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "class Log:\r\n",
        "    LOG, ERROR = 0, 1\r\n",
        "\r\n",
        "    def __init__(self, outfile='data/.log', filename='data/logfile.log'):\r\n",
        "        self.filename = filename\r\n",
        "        self.outfile = outfile\r\n",
        "        self.file_object = open(filename, 'a+', encoding='utf-8')\r\n",
        "        self.line_num = 0\r\n",
        "        print(\"LOGGING For seesion on: \" + str(datetime.now()), file=self.file_object)\r\n",
        "\r\n",
        "    def print(self, txt, type=LOG, shell=True):\r\n",
        "        if shell: print(txt)\r\n",
        "        prefix = \"LOG ::\" if type==Log.LOG else \"ERROR ::\"\r\n",
        "        txt = f\"{prefix} {str(datetime.now())} :: {txt}\"\r\n",
        "        print(txt, file=self.file_object)\r\n",
        "\r\n",
        "    def close(self):\r\n",
        "        self.file_object.seek(0, 0)\r\n",
        "        text = self.file_object.read()\r\n",
        "        text = text.split('\\n')\r\n",
        "        text.reverse()\r\n",
        "        output = '\\n'.join(text)\r\n",
        "        self.file_object.close()\r\n",
        "\r\n",
        "        with open(self.outfile, 'w') as f:\r\n",
        "            f.write(output)\r\n",
        "\r\n",
        "    def flush(self):\r\n",
        "        self.file_object.close()\r\n",
        "        self.file_object = open(self.filename, 'a+', encoding = 'utf-8')\r\n",
        "\r\n",
        "Log().close()\r\n",
        "log = Log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_z9pVI7Jce5",
        "outputId": "e389420e-11b4-4d10-f06e-d42cb1f31388"
      },
      "source": [
        "from datetime import datetime\r\n",
        "print(datetime.now())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-18 09:17:11.854725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6phs1gw08j8b",
        "outputId": "13a432ae-139e-4429-ffd7-6e19e869c0ad"
      },
      "source": [
        "cd /content/drive/MyDrive/Dissertation/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Dissertation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtC7_kj6vy09"
      },
      "source": [
        "class Opt:\r\n",
        "    pass\r\n",
        "opt = Opt()\r\n",
        "opt.src_lang = 'es'\r\n",
        "opt.trg_lang = 'en'\r\n",
        "opt.num_mil = 1\r\n",
        "opt.input_file = f'data/{opt.src_lang}/ParaCrawl.{opt.src_lang}-{opt.trg_lang}.'\r\n",
        "opt.model_file = f'data/{opt.src_lang}/SPM-{opt.num_mil}m-8k.{opt.src_lang}-{opt.trg_lang}.'\r\n",
        "opt.interim_file = f'data/{opt.src_lang}/ParaCrawl.{opt.src_lang}-{opt.trg_lang}.{opt.num_mil}m.'\r\n",
        "opt.dataset = f'data/{opt.src_lang}/tokenized_dataset_{opt.src_lang}_{opt.num_mil}m'\r\n",
        "opt.dev_dataset = f'data/{opt.src_lang}/DEV-{opt.src_lang}-{opt.trg_lang}.'\r\n",
        "# opt.model_file = 'data/SPM-1m-8k.fr-en.'\r\n",
        "opt.max_len = 150\r\n",
        "opt.dev_dataset = f'data/{opt.src_lang}/DEV-{opt.src_lang}-{opt.trg_lang}.'\r\n",
        "\r\n",
        "\r\n",
        "opt.src_data_path = opt.interim_file + opt.src_lang\r\n",
        "opt.trg_data_path = opt.interim_file + opt.trg_lang\r\n",
        "opt.vocab_size = 8000\r\n",
        "opt.tokensize = 2048\r\n",
        "opt.print_every = 200\r\n",
        "opt.save_every = 5000\r\n",
        "opt.epochs = 10\r\n",
        "opt.warmup_steps = 16000\r\n",
        "opt.keep_training = False\r\n",
        "\r\n",
        "opt.path = f'{opt.src_lang}-en-models'\r\n",
        "opt.model_prefix = f'{opt.src_lang}-en-model-'\r\n",
        "# optim_file = 'data/optim_state_dict'\r\n",
        "opt.model_dim = 512\r\n",
        "opt.heads = 8\r\n",
        "opt.N = 6\r\n",
        "opt.args = (opt.vocab_size, opt.vocab_size, \r\n",
        "            opt.model_dim, opt.model_dim*4, \r\n",
        "            opt.heads, opt.N, opt.max_len, 0.1, True)\r\n",
        "opt.log = log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlPP7DUC6Std"
      },
      "source": [
        "def move(opt):\r\n",
        "    def move_lang(lang):\r\n",
        "        inpFile = open(opt.input_file + lang, 'r', encoding='utf-8')\r\n",
        "        intFile = open(opt.interim_file + lang, 'w', encoding='utf-8')\r\n",
        "\r\n",
        "        for i in tnrange(int(opt.num_mil * 10**6)):\r\n",
        "            intFile.write(inpFile.readline())\r\n",
        "        inpFile.close()\r\n",
        "        intFile.close()\r\n",
        "    move_lang(opt.src_lang)\r\n",
        "    move_lang(opt.trg_lang)\r\n",
        "# move(opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWwdmk8y6IjI"
      },
      "source": [
        "# ! spm_train --input=data/es/ParaCrawl.es-en.1m.es --model_prefix=data/es/SPM-1m-8k.es-en.es --vocab_size=8000 --character_coverage=1.0 --model_type=bpe --pad_id=3 --bos_id=-1 --eos_id=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuVsOTLR9QI8"
      },
      "source": [
        "# ! spm_train --input=data/es/ParaCrawl.es-en.1m.en --model_prefix=data/es/SPM-1m-8k.es-en.en --vocab_size=8000 --character_coverage=1.0 --model_type=bpe --pad_id=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPfCeZUxw7XJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a37079-2a0b-4fda-c474-2b7a8325921f"
      },
      "source": [
        "def create_fields(opt):\r\n",
        "    \r\n",
        "    print('creating fields')\r\n",
        "    \r\n",
        "    if not os.path.exists(f\"{opt.model_file}{opt.src_lang}.model\"):\r\n",
        "        print('building src processor') \r\n",
        "        raise Exception\r\n",
        "\r\n",
        "    \r\n",
        "    if not os.path.exists(f\"{opt.model_file}{opt.trg_lang}.model\"):\r\n",
        "        print('building trg processor')\r\n",
        "        raise Exception\r\n",
        "\r\n",
        "    print(\"initlizing sentence processors\")\r\n",
        "    opt.src_processor = spm.SentencePieceProcessor()\r\n",
        "    opt.src_processor.Init(model_file=f'{opt.model_file}{opt.src_lang}.model')\r\n",
        "    opt.trg_processor = spm.SentencePieceProcessor()\r\n",
        "    opt.trg_processor.Init(model_file=f'{opt.model_file}{opt.trg_lang}.model')\r\n",
        "\r\n",
        "    opt.src_pad = opt.src_processor.pad_id()\r\n",
        "    opt.trg_pad = opt.trg_processor.pad_id()\r\n",
        "    opt.trg_bos = opt.trg_processor.bos_id()\r\n",
        "    opt.trg_eos = opt.trg_processor.eos_id()\r\n",
        "\r\n",
        "\r\n",
        "def create_dataset(opt):\r\n",
        "\r\n",
        "    opt.bins = [i for i in range(10, opt.max_len+1)]\r\n",
        "\r\n",
        "    if opt.dataset is not None and os.path.exists(opt.dataset):\r\n",
        "        print('loading saved dataset...')\r\n",
        "        with open(opt.dataset, 'rb') as f:\r\n",
        "            opt.src_bins = pickle.load(f)\r\n",
        "            opt.trg_bins = pickle.load(f)\r\n",
        "\r\n",
        "        print({s: len(opt.src_bins[s]) for s in opt.bins})\r\n",
        "        return\r\n",
        "\r\n",
        "    print('reading datasets')\r\n",
        "    with open(opt.src_data_path, 'r', encoding='utf-8') as f:\r\n",
        "        opt.src_data = f.read().split('\\n')\r\n",
        "    with open(opt.trg_data_path, 'r', encoding='utf-8') as f:\r\n",
        "        opt.trg_data = f.read().split('\\n')\r\n",
        "\r\n",
        "    opt.src_bins = {i:[] for i in opt.bins}\r\n",
        "    opt.trg_bins = {i:[] for i in opt.bins}\r\n",
        "\r\n",
        "    print('tokenizing and bining...')\r\n",
        "    for i in tnrange(len(opt.src_data)):\r\n",
        "        src = opt.src_data[i]\r\n",
        "        trg = opt.trg_data[i]\r\n",
        "    # for i, (src, trg) in enumerate(zip(opt.src_data, opt.trg_data)):\r\n",
        "        src = opt.src_processor.encode(src)\r\n",
        "        trg = [opt.trg_bos] + opt.trg_processor.encode(trg) + [opt.trg_eos]\r\n",
        "        opt.src_data[i] = 0\r\n",
        "        opt.trg_data[i] = 0\r\n",
        "\r\n",
        "        lsrc= len(src)\r\n",
        "        ltrg = len(trg)\r\n",
        "        if lsrc > opt.max_len or ltrg > opt.max_len:\r\n",
        "            continue\r\n",
        "        \r\n",
        "        for v in opt.bins:\r\n",
        "            if lsrc <= v and ltrg <= v:\r\n",
        "                for i in range(lsrc, v):\r\n",
        "                    src.append(opt.src_pad)\r\n",
        "                for i in range(ltrg, v):\r\n",
        "                    trg.append(opt.trg_pad)\r\n",
        "                \r\n",
        "                opt.src_bins[v].append(src)\r\n",
        "                opt.trg_bins[v].append(trg)\r\n",
        "                break\r\n",
        "\r\n",
        "    if opt.dataset is not None:\r\n",
        "        with open(opt.dataset, 'wb') as f:\r\n",
        "            pickle.dump(opt.src_bins, f)\r\n",
        "            pickle.dump(opt.trg_bins, f)\r\n",
        "\r\n",
        "    print({s: len(opt.src_bins[s]) for s in opt.bins})\r\n",
        "\r\n",
        "create_fields(opt)\r\n",
        "create_dataset(opt)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating fields\n",
            "initlizing sentence processors\n",
            "loading saved dataset...\n",
            "{10: 36, 11: 9, 12: 20, 13: 38, 14: 28, 15: 43, 16: 25, 17: 27, 18: 22, 19: 15, 20: 6, 21: 8, 22: 9, 23: 11, 24: 23, 25: 23, 26: 291, 27: 676, 28: 636, 29: 682, 30: 1408, 31: 2392, 32: 3731, 33: 7784, 34: 8105, 35: 11320, 36: 11992, 37: 13262, 38: 15144, 39: 16658, 40: 18891, 41: 19362, 42: 21538, 43: 21926, 44: 24012, 45: 24913, 46: 24096, 47: 24438, 48: 24528, 49: 24098, 50: 23286, 51: 21698, 52: 20120, 53: 17725, 54: 16078, 55: 14397, 56: 12427, 57: 11141, 58: 9615, 59: 8435, 60: 7671, 61: 6594, 62: 6348, 63: 5375, 64: 4922, 65: 4320, 66: 3943, 67: 3725, 68: 3454, 69: 3245, 70: 3279, 71: 3244, 72: 3174, 73: 3802, 74: 3842, 75: 3293, 76: 3382, 77: 3573, 78: 3974, 79: 3992, 80: 4210, 81: 4381, 82: 4616, 83: 4916, 84: 6176, 85: 5964, 86: 5940, 87: 5527, 88: 5678, 89: 5716, 90: 5824, 91: 6204, 92: 7026, 93: 6693, 94: 6590, 95: 6980, 96: 7998, 97: 7366, 98: 7461, 99: 7594, 100: 8313, 101: 8022, 102: 7863, 103: 7985, 104: 8718, 105: 8214, 106: 8816, 107: 8532, 108: 8609, 109: 8111, 110: 7994, 111: 7923, 112: 8014, 113: 7739, 114: 7067, 115: 7123, 116: 7112, 117: 7199, 118: 6760, 119: 6600, 120: 6233, 121: 6335, 122: 6008, 123: 5869, 124: 5674, 125: 5702, 126: 5245, 127: 5288, 128: 5375, 129: 4883, 130: 4750, 131: 4595, 132: 4431, 133: 4258, 134: 4119, 135: 3951, 136: 3796, 137: 3549, 138: 3410, 139: 3404, 140: 3083, 141: 2969, 142: 2950, 143: 2771, 144: 2482, 145: 2539, 146: 2455, 147: 2224, 148: 2166, 149: 2010, 150: 2000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AjOQPKmPSwS"
      },
      "source": [
        "opt.dev_dataset = f'data/{opt.src_lang}/DEV-{opt.src_lang}-{opt.trg_lang}.'\r\n",
        "\r\n",
        "with open(opt.dev_dataset + opt.src_lang, 'r', encoding='utf-8') as f:\r\n",
        "    opt.dev_src_sentences = f.read().split('\\n')[:2000]\r\n",
        "with open(opt.dev_dataset + opt.trg_lang, 'r', encoding='utf-8') as f:\r\n",
        "    opt.dev_trg_sentences = f.read().split('\\n')[:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noEKiuKy1vg3",
        "outputId": "d40a29f2-ca23-4ac7-90f4-218211d66b5b"
      },
      "source": [
        "opt.train_len = sum((len(opt.src_bins[v])*v) // opt.tokensize + 1 for v in opt.bins) \r\n",
        "print(opt.train_len)\r\n",
        "print(sum(len(opt.src_bins[v]) for v in opt.bins))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35390\n",
            "966398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f38B9vQL7vLc"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "class Save:\r\n",
        "    def __init__(self, model_state_dict=None, optim_state_dict=None):\r\n",
        "        self.model_state_dict = model_state_dict\r\n",
        "        self.optim_state_dict = optim_state_dict\r\n",
        "\r\n",
        "    def save(self, filename):\r\n",
        "        torch.save(self.model_state_dict, f'{filename}.model')\r\n",
        "        torch.save(self.optim_state_dict, f'{filename}.optim')\r\n",
        "    \r\n",
        "    def load(self, filename):\r\n",
        "        \r\n",
        "        if torch.cuda.is_available():\r\n",
        "            device = torch.device('cuda:0')\r\n",
        "        else:\r\n",
        "            device = torch.device('cpu')\r\n",
        "\r\n",
        "        self.model_state_dict = torch.load(f'{filename}.model', map_location=device)\r\n",
        "        self.optim_state_dict = torch.load(f'{filename}.optim', map_location=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FeZOdmMMIVn"
      },
      "source": [
        "def batch(opt):\r\n",
        "    max_count = {v:(len(opt.src_bins[v])*v) // opt.tokensize for v in opt.bins}\r\n",
        "    # print(max_count)\r\n",
        "    cur_count = {v:0 for v in opt.bins}\r\n",
        "    batch_sizes = {v: opt.tokensize // v for v in opt.bins}\r\n",
        "\r\n",
        "    step = 0\r\n",
        "    while (len(max_count)):\r\n",
        "        v = np.random.choice(list(max_count.keys()))\r\n",
        "        i = cur_count[v]\r\n",
        "        cur_count[v] += 1\r\n",
        "        j = cur_count[v]\r\n",
        "\r\n",
        "        step += 1\r\n",
        "        if step < opt.starting_index: continue\r\n",
        "\r\n",
        "        size = batch_sizes[v]\r\n",
        "        src_list = opt.src_bins[v][i*size: j*size]\r\n",
        "        trg_list = opt.trg_bins[v][i*size: j*size]\r\n",
        "\r\n",
        "        if j > max_count[v]:\r\n",
        "            if opt.keep_training: cur_count[v] = 0\r\n",
        "            else: del max_count[v]\r\n",
        "\r\n",
        "        if len(src_list) == 0:\r\n",
        "            continue\r\n",
        "\r\n",
        "        yield src_list, trg_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCbJUh9CAgFf"
      },
      "source": [
        "def nopeak_mask(size, opt):\r\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\r\n",
        "    k=1).astype('uint8')\r\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0).to(device)\r\n",
        "    return np_mask\r\n",
        "\r\n",
        "def create_masks(src, trg, opt):\r\n",
        "    \r\n",
        "    src_mask = (src != opt.src_pad).unsqueeze(-2)\r\n",
        "\r\n",
        "    if trg is not None:\r\n",
        "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\r\n",
        "        size = trg.size(1) # get seq_len for matrix\r\n",
        "        np_mask = nopeak_mask(size, opt).to(device)\r\n",
        "        trg_mask = trg_mask & np_mask\r\n",
        "        \r\n",
        "    else:\r\n",
        "        trg_mask = None\r\n",
        "    return src_mask, trg_mask\r\n",
        "\r\n",
        "def init_vars(src, model, opt):\r\n",
        "    \r\n",
        "    init_tok = opt.trg_bos\r\n",
        "    src_mask = (src != opt.src_pad).unsqueeze(-2)\r\n",
        "    e_output = model.encoder(src, src_mask)\r\n",
        "    \r\n",
        "    outputs = torch.LongTensor([[init_tok]]).to(opt.device)\r\n",
        "    \r\n",
        "    trg_mask = nopeak_mask(1, opt)\r\n",
        "    \r\n",
        "    out = model.linear(model.decoder(outputs, e_output, src_mask, trg_mask))\r\n",
        "    out = F.softmax(out, dim=-1)\r\n",
        "    \r\n",
        "    probs, ix = out[:, -1].data.topk(opt.k)\r\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\r\n",
        "    \r\n",
        "    outputs = torch.zeros(opt.k, opt.max_len).long().to(opt.device)\r\n",
        "    outputs[:, 0] = init_tok\r\n",
        "    outputs[:, 1] = ix[0]\r\n",
        "    \r\n",
        "    e_outputs = torch.zeros(opt.k, e_output.size(-2),e_output.size(-1)).to(opt.device)\r\n",
        "    e_outputs[:, :] = e_output[0]\r\n",
        "    \r\n",
        "    return outputs, e_outputs, log_scores\r\n",
        "\r\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\r\n",
        "    \r\n",
        "    probs, ix = out[:, -1].data.topk(k)\r\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\r\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\r\n",
        "    \r\n",
        "    row = k_ix // k\r\n",
        "    col = k_ix % k\r\n",
        "\r\n",
        "    outputs[:, :i] = outputs[row, :i]\r\n",
        "    outputs[:, i] = ix[row, col]\r\n",
        "\r\n",
        "    log_scores = k_probs.unsqueeze(0)\r\n",
        "    \r\n",
        "    return outputs, log_scores\r\n",
        "\r\n",
        "def beam_search(src, model, opt):\r\n",
        "    \r\n",
        "\r\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model, opt)\r\n",
        "    eos_tok = opt.trg_eos\r\n",
        "    src_mask = (src != opt.src_pad).unsqueeze(-2)\r\n",
        "    ind = None\r\n",
        "    for i in range(2, opt.max_len):\r\n",
        "    \r\n",
        "        trg_mask = nopeak_mask(i, opt)\r\n",
        "\r\n",
        "        out = model.linear(model.decoder(outputs[:,:i],\r\n",
        "        e_outputs, src_mask, trg_mask))\r\n",
        "\r\n",
        "        out = F.softmax(out, dim=-1)\r\n",
        "    \r\n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, opt.k)\r\n",
        "        \r\n",
        "        ones = torch.nonzero(outputs == eos_tok)\r\n",
        "        # ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\r\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\r\n",
        "        for vec in ones:\r\n",
        "            i = vec[0]\r\n",
        "            if sentence_lengths[i]==0: # First end symbol has not been found yet\r\n",
        "                sentence_lengths[i] = vec[1] # Position of first end symbol\r\n",
        "\r\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\r\n",
        "\r\n",
        "        if num_finished_sentences == opt.k:\r\n",
        "            alpha = 0.7\r\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\r\n",
        "            _, ind = torch.max(log_scores * div, 1)\r\n",
        "            ind = ind.data[0]\r\n",
        "            break\r\n",
        "    \r\n",
        "    if ind is None:\r\n",
        "        # print(outputs)\r\n",
        "        length = (outputs[0]==eos_tok).nonzero()[0]\r\n",
        "        sentence_list = (outputs[0][1:length]).tolist()\r\n",
        "        return ''.join(opt.trg_processor.decode(sentence_list)).replace('_', \" \")\r\n",
        "    \r\n",
        "    else:\r\n",
        "        length = (outputs[ind]==eos_tok).nonzero()[0]\r\n",
        "        sentence_list = (outputs[0][1:length]).tolist()\r\n",
        "        return ''.join(opt.trg_processor.decode(sentence_list)).replace('_', \" \")\r\n",
        "\r\n",
        "import re\r\n",
        "def multiple_replace(dict, text):\r\n",
        "  # Create a regular expression  from the dictionary keys\r\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\r\n",
        "\r\n",
        "  # For each match, look-up corresponding value in dictionary\r\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) \r\n",
        "\r\n",
        "def translate_sentence(sentence, model, opt):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    sentence = Variable(torch.LongTensor([opt.src_processor.encode(sentence)])).to(opt.device)\r\n",
        "    \r\n",
        "    sentence = beam_search(sentence, model, opt)\r\n",
        "\r\n",
        "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z0-ADZgK-Mg"
      },
      "source": [
        "if torch.cuda.is_available():\r\n",
        "    dev = \"cuda:0\"\r\n",
        "else:\r\n",
        "    dev = \"cpu\"\r\n",
        "\r\n",
        "device = torch.device(dev)\r\n",
        "opt.device = device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQDp0d4_SITf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wnfVW9CxKQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cfbe8c-9202-48be-af94-7ac7a2258543"
      },
      "source": [
        "model = Transformer(*args)\r\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\r\n",
        "opt.save_model = Save()\r\n",
        "starting_index = 0\r\n",
        "for p in model.parameters():\r\n",
        "    if p.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(p)\r\n",
        "\r\n",
        "# initializing the parameters of the model.\r\n",
        "\r\n",
        "if not os.path.exists(path):\r\n",
        "    if not os.path.exists(path):\r\n",
        "        os.mkdir(path)\r\n",
        "    log.print(f\"No {path} found. Created a new path directory and started using xavier_uniform\")\r\n",
        "else:\r\n",
        "    for i in os.walk(path):\r\n",
        "        break\r\n",
        "    i = i[2]\r\n",
        "    m = 0\r\n",
        "    mf = None\r\n",
        "    suffix_len = len('.model')\r\n",
        "    for file in i:\r\n",
        "        if model_prefix not in file or '.model' not in file: continue\r\n",
        "        \r\n",
        "        num = int(file[len(model_prefix):-suffix_len])\r\n",
        "        if num > m:\r\n",
        "            m = num\r\n",
        "            mf = file[:-suffix_len]\r\n",
        "                \r\n",
        "    if mf is not None:\r\n",
        "        log.print(f\"Starting from last saved {mf}\")\r\n",
        "        opt.save_model.load(f'{path}/{mf}')\r\n",
        "        model.load_state_dict(opt.save_model.model_state_dict)\r\n",
        "        optim.load_state_dict(opt.save_model.optim_state_dict)\r\n",
        "        starting_index = m\r\n",
        "    else:\r\n",
        "        log.print(f\"Starting from xavier_uniform distribution\")\r\n",
        "opt.starting_index = starting_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting from last saved es-en-model-120000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2YIfqdJ8vM_"
      },
      "source": [
        "import math\r\n",
        "def evaluate(model, opt):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # translated = []\r\n",
        "    start = time.time()\r\n",
        "\r\n",
        "    refs = []\r\n",
        "    hyp = []\r\n",
        "    opt.skip = []\r\n",
        "\r\n",
        "    # print(len(opt.dev_src_sentences))\r\n",
        "    tk2 = tnrange(len(opt.dev_src_sentences))\r\n",
        "    for i in tk2:\r\n",
        "        sentence = opt.dev_src_sentences[i]\r\n",
        "        try:\r\n",
        "            translated = translate_sentence(sentence.lower(), model, opt)\r\n",
        "        except Exception as e:\r\n",
        "            opt.skip.append(i)\r\n",
        "            # raise e\r\n",
        "        else:\r\n",
        "            refs.append(opt.dev_trg_sentences[i])\r\n",
        "            hyp.append(translated)\r\n",
        "            if i < 10:\r\n",
        "                hyp_print = f\"hyp: {hyp[-1]}\"\r\n",
        "                ref_print = f\"ref: {refs[-1]}\"\r\n",
        "                log.print(hyp_print, shell=False)\r\n",
        "                log.print(ref_print, shell=False)\r\n",
        "                log.print('', shell=False)\r\n",
        "                log.flush()\r\n",
        "    # print()\r\n",
        "    \r\n",
        "        if i == len(opt.dev_src_sentences) - 1:\r\n",
        "            opt.hyp = hyp\r\n",
        "            opt.refs = refs\r\n",
        "\r\n",
        "            bleu = sacrebleu.corpus_bleu(opt.hyp[:-1], [opt.refs[:-1]])\r\n",
        "            ter = sacrebleu.corpus_ter(opt.hyp[:-1], [opt.refs[:-1]])\r\n",
        "            tk2.set_postfix_str(f\"({ter} || {bleu})\")\r\n",
        "            log.print(f'{ter} || {bleu}', shell=False)\r\n",
        "    return bleu, ter\r\n",
        "\r\n",
        "# opt.k = 10\r\n",
        "# evaluate(model, opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1hW2b8KxUey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "4d99fbecb4b64dc5b9c5d9494ca5757d",
            "b5e3959c47134fab9a784acca8d83e82",
            "6070faad522144dda072d0341830913e",
            "e6d46ac6f426440c8de9f78ea3699e6f",
            "e0583df15e8948e1bdda5703d32a30b1",
            "c4cdc0b3718a4ed9bb3c7b1881bacd2c",
            "8de8639f38884c7eae7a5fac62d15361",
            "c14436d66d3f49df8e71a79c762d6018",
            "113f0d6c130246368672054e82fd4bda",
            "38d48f3db137434095f051c534cb65b7",
            "3e3c4cee871c41b5984b34c6881bfc91",
            "3a1238139a204c96accede8c6e505f83",
            "d0e2768bea3f4b2a955cdd4647d23a7e",
            "c450486bc52a4e3399d742219e50c326",
            "6a869b0d37934bc38ba66a9838a660a3",
            "6a3d181d6ee740a1ad6248445cc97553"
          ]
        },
        "outputId": "93c2540d-7b55-4f50-9702-7d764606750d"
      },
      "source": [
        "log.flush()\n",
        "k = 10\n",
        "# to adjust for models already present\n",
        "opt.device = device\n",
        "def train_model(model, opt):\n",
        "    model.train()\n",
        " \n",
        "    # starting_index += _starting_index\n",
        " \n",
        "    start = time.time()\n",
        "    \n",
        "    total_loss = 0\n",
        "    total_loss_at_save = 0\n",
        "    last_loss = 0\n",
        "    r = opt.save_every / opt.print_every\n",
        " \n",
        "    prev_scores = []\n",
        "    tk0 = tnrange(opt.epochs)\n",
        "    for epoch in tk0:\n",
        "        temp = time.time()\n",
        "        tk1 = tnrange(opt.train_len)\n",
        "        batch_gen = batch(opt)\n",
        "        last_batch_loss = 0\n",
        "        for i in tk1:\n",
        "            src_lis, trg_lis = next(batch_gen)\n",
        "            optim.zero_grad()\n",
        "            try:\n",
        "                src_tensor = torch.LongTensor(src_lis).to(opt.device)\n",
        "                src_tensor.requires_grad = False\n",
        "                trg_np = np.array(trg_lis)\n",
        "                trg_tensor = torch.LongTensor(trg_np[:, :-1]).to(device)\n",
        "                trg_tensor.requires_grad = False\n",
        "            except Exception as e:\n",
        "                del src_tensor\n",
        "                continue\n",
        "            src_mask, trg_mask = create_masks(src_tensor, trg_tensor, opt)\n",
        " \n",
        "            preds = model(src_tensor, trg_tensor, src_mask, trg_mask)\n",
        "            target = torch.LongTensor(trg_np[:, 1:]).to(device).contiguous().view(-1)\n",
        "            preds = preds.view(-1, preds.size(-1))\n",
        "            loss = F.cross_entropy(preds, target, ignore_index=opt.trg_pad)\n",
        " \n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "            optim.step()\n",
        "            step = starting_index + int(epoch * opt.train_len) + i + 1\n",
        "            optim.param_groups[0]['lr'] = (model_dim ** (-0.5)) * min(step ** (-0.5), step * (opt.warmup_steps ** (-1.5)))\n",
        " \n",
        "            del src_mask, src_tensor, trg_mask, trg_tensor, preds, loss\n",
        "            torch.cuda.empty_cache()\n",
        " \n",
        "            if (step) % opt.print_every == 0:\n",
        "                diff = total_loss - last_batch_loss\n",
        "                diff = '%.3f'%diff\n",
        "                last_batch_loss = total_loss\n",
        "\n",
        "                avg = \"%.3f\" % (total_loss)\n",
        "                t = \"%.3f\" % (time.time() - temp)\n",
        "                tt = time.strftime('%H:%M:%S', time.gmtime(time.time() - start))\n",
        "\n",
        "                output = f\"time: {t}s, total: {tt}, loss = {avg}, step = {step}, diff = {diff}\"\n",
        "                log.print(output, shell=False)\n",
        "                tk1.set_postfix_str(\", \" + output + '\\n')\n",
        " \n",
        "                total_loss_at_save += total_loss\n",
        "                total_loss = 0\n",
        "                temp = time.time()\n",
        " \n",
        "            if (step) % opt.save_every == 0:\n",
        "                model_name = f'{path}/{model_prefix}{step}'\n",
        "                opt.save_model.model_state_dict = model.state_dict()\n",
        "                opt.save_model.optim_state_dict = optim.state_dict()\n",
        "                opt.save_model.save(model_name)\n",
        " \n",
        "                avg = (total_loss_at_save / r)\n",
        "                total_loss_at_save = 0\n",
        "                diff = avg - last_loss\n",
        "                last_loss = avg\n",
        " \n",
        "                avg = '%.3f' % avg\n",
        "                diff = '%.3f' % diff\n",
        "                output = f\"Saving model: {model_name} | avg_loss: {avg} | diff: {diff}\"\n",
        "                log.print(output, shell=False)\n",
        "                tk0.set_postfix_str(',' + output + '\\n')            \n",
        " \n",
        "            if step % opt.train_len == 0:\n",
        "                bleu_score, ter = evaluate(model, opt)\n",
        "                score = '%.3f' % bleu_score.score\n",
        " \n",
        "                prev_scores.append(bleu_score.score)\n",
        "                if len(prev_scores) > 5: prev_scores.pop(0)\n",
        " \n",
        "                prev_avg_score = sum(prev_scores) / len(prev_scores)\n",
        " \n",
        "                # if len(prev_scores) == 5 and prev_avg_score - bleu_score.score < epsilon:\n",
        "                #     break\n",
        "                    \n",
        "                log.print(f\"{ter} || {bleu_score}\")\n",
        "                temp = time.time()\n",
        "                model.train()\n",
        " \n",
        "        log.flush()\n",
        " \n",
        " \n",
        "if __name__ == '__main__':\n",
        " \n",
        "    try:\n",
        "        # print(eval())\n",
        "        opt.k = 10\n",
        "        opt.print_every = 200 \n",
        "        opt.save_every = 5000\n",
        "        opt.epochs = 10\n",
        "        opt.warmup_steps = 16000\n",
        "        opt.keep_training = True\n",
        "        train_model(model, opt)\n",
        " \n",
        "    except Exception as e:\n",
        "        log.print(e, type=Log.ERROR)\n",
        "        log.flush()\n",
        "        raise e"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d99fbecb4b64dc5b9c5d9494ca5757d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "113f0d6c130246368672054e82fd4bda",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=35390.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}